{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUU5M2p+/UgihJtUIM6Ibg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keerthisree01/29_Bellamkonda_Keerthisree/blob/main/AI_Assistance_Customer_Service.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU3q_5UCKw9I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eb14ad3"
      },
      "source": [
        "# Task\n",
        "Build an AI Customer Service Agent using RAG and escalation rules, based on the customer interaction data in 'CustomerInteractionData.csv'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0da0c4c6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/CustomerInteractionData.csv')\n",
        "\n",
        "print(\"First 5 rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nColumn names:\")\n",
        "print(df.columns)\n",
        "\n",
        "print(\"\\nDataFrame Information:\")\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30fdd84e"
      },
      "source": [
        "import re\n",
        "\n",
        "# Extract the 'CustomerInteractionRawText' column\n",
        "raw_text = df['CustomerInteractionRawText']\n",
        "\n",
        "# Clean the extracted text\n",
        "cleaned_text = raw_text.str.lower()\n",
        "# Remove special characters, punctuation, and numbers\n",
        "cleaned_text = cleaned_text.apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
        "# Remove extra whitespace and trim leading/trailing spaces\n",
        "cleaned_text = cleaned_text.apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
        "\n",
        "df['CleanedInteractionText'] = cleaned_text\n",
        "\n",
        "print(\"Original vs. Cleaned Text (first 5 rows):\")\n",
        "for i in range(5):\n",
        "    print(f\"Original: {df['CustomerInteractionRawText'].iloc[i]}\")\n",
        "    print(f\"Cleaned: {df['CleanedInteractionText'].iloc[i]}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f692a62e"
      },
      "source": [
        "def chunk_text(text, chunk_size=100):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunks.append(' '.join(words[i:i + chunk_size]))\n",
        "    return chunks\n",
        "\n",
        "# Apply the chunking function to the 'CleanedInteractionText' column\n",
        "df['TextChunks'] = df['CleanedInteractionText'].apply(chunk_text)\n",
        "\n",
        "print(\"First 5 rows of DataFrame with Text Chunks:\")\n",
        "print(df[['CleanedInteractionText', 'TextChunks']].head())\n",
        "\n",
        "print(\"\\nExample of a chunked interaction (first entry):\\n\")\n",
        "for i, chunk in enumerate(df['TextChunks'].iloc[0]):\n",
        "    print(f\"Chunk {i+1}: {chunk}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "480e5b5b"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a pre-trained sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Initialize empty lists to store all individual text chunks and their corresponding embeddings\n",
        "all_text_chunks = []\n",
        "all_chunk_embeddings = []\n",
        "\n",
        "print(\"Sentence Transformer model loaded successfully and lists initialized.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "659f1dc2"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install faiss-cpu\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Clear and re-populate lists to ensure a fresh start\n",
        "all_text_chunks = []\n",
        "all_chunk_embeddings = []\n",
        "\n",
        "# Populate all_text_chunks from the DataFrame\n",
        "for chunks_list in df['TextChunks']:\n",
        "    all_text_chunks.extend(chunks_list)\n",
        "\n",
        "# Encode all collected text chunks into numerical vector embeddings\n",
        "all_chunk_embeddings = model.encode(all_text_chunks, show_progress_bar=True)\n",
        "\n",
        "# Get the dimension of the embeddings\n",
        "embedding_dimension = all_chunk_embeddings.shape[1]\n",
        "\n",
        "# Create a FAISS index (e.g., IndexFlatL2 for L2 distance)\n",
        "index = faiss.IndexFlatL2(embedding_dimension)\n",
        "\n",
        "# Add the generated embeddings to the index\n",
        "index.add(np.array(all_chunk_embeddings).astype('float32'))\n",
        "\n",
        "print(f\"Total text chunks: {len(all_text_chunks)}\")\n",
        "print(f\"Shape of all_chunk_embeddings: {all_chunk_embeddings.shape}\")\n",
        "print(f\"FAISS index contains {index.ntotal} vectors.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d228d61"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install faiss-cpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fac4f393"
      },
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Populate all_text_chunks from the DataFrame\n",
        "for chunks_list in df['TextChunks']:\n",
        "    all_text_chunks.extend(chunks_list)\n",
        "\n",
        "# Encode all collected text chunks into numerical vector embeddings\n",
        "all_chunk_embeddings = model.encode(all_text_chunks, show_progress_bar=True)\n",
        "\n",
        "# Get the dimension of the embeddings\n",
        "embedding_dimension = all_chunk_embeddings.shape[1]\n",
        "\n",
        "# Create a FAISS index (e.g., IndexFlatL2 for L2 distance)\n",
        "index = faiss.IndexFlatL2(embedding_dimension)\n",
        "\n",
        "# Add the generated embeddings to the index\n",
        "index.add(np.array(all_chunk_embeddings).astype('float32'))\n",
        "\n",
        "print(f\"Total text chunks: {len(all_text_chunks)}\")\n",
        "print(f\"Shape of all_chunk_embeddings: {all_chunk_embeddings.shape}\")\n",
        "print(f\"FAISS index contains {index.ntotal} vectors.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88232517"
      },
      "source": [
        "def simulate_ask(user_query, top_k=3):\n",
        "    # 1. Embed the user query\n",
        "    query_embedding = model.encode([user_query])\n",
        "    query_embedding = np.array(query_embedding).astype('float32')\n",
        "\n",
        "    # 2. Perform similarity search\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    # 3. Retrieve relevant text chunks (source IDs)\n",
        "    retrieved_context = []\n",
        "    source_ids = []\n",
        "    for i in indices[0]:\n",
        "        if i < len(all_text_chunks): # Ensure index is within bounds\n",
        "            retrieved_context.append(all_text_chunks[i])\n",
        "            source_ids.append(i)\n",
        "\n",
        "    # 4. Simulate LLM response\n",
        "    if retrieved_context:\n",
        "        generated_answer = f\"Based on your question: '{user_query}', and the following context: [{'; '.join(retrieved_context)}], the AI assistant says: This is a simulated response based on the relevant information found.\"\n",
        "    else:\n",
        "        generated_answer = f\"Based on your question: '{user_query}', the AI assistant says: I couldn't find relevant information in my knowledge base. Please try rephrasing your question.\"\n",
        "\n",
        "    return generated_answer, source_ids\n",
        "\n",
        "print(\"The 'simulate_ask' function has been defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2717f6ae"
      },
      "source": [
        "user_query = \"What is the customer's issue regarding porting out?\"\n",
        "answer, sources = simulate_ask(user_query)\n",
        "\n",
        "print(f\"User Query: {user_query}\")\n",
        "print(f\"Generated Answer: {answer}\")\n",
        "print(f\"Source IDs: {sources}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97fa965f"
      },
      "source": [
        "escalation_keywords = ['escalate', 'urgent', 'manager', 'complaint']\n",
        "\n",
        "def simulate_ask(user_query, top_k=3):\n",
        "    escalation_required = False\n",
        "\n",
        "    # Check for escalation keywords in the user query\n",
        "    for keyword in escalation_keywords:\n",
        "        if keyword in user_query.lower():\n",
        "            escalation_required = True\n",
        "            break\n",
        "\n",
        "    # 1. Embed the user query\n",
        "    query_embedding = model.encode([user_query])\n",
        "    query_embedding = np.array(query_embedding).astype('float32')\n",
        "\n",
        "    # 2. Perform similarity search\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    # 3. Retrieve relevant text chunks (source IDs)\n",
        "    retrieved_context = []\n",
        "    source_ids = []\n",
        "    for i in indices[0]:\n",
        "        if i < len(all_text_chunks): # Ensure index is within bounds\n",
        "            retrieved_context.append(all_text_chunks[i])\n",
        "            source_ids.append(i)\n",
        "\n",
        "    # 4. Simulate LLM response and handle escalation if no context found\n",
        "    if not retrieved_context:\n",
        "        escalation_required = True\n",
        "        generated_answer = f\"Based on your question: '{user_query}', the AI assistant says: I couldn't find relevant information in my knowledge base. This query requires escalation due to lack of relevant information.\"\n",
        "    else:\n",
        "        generated_answer = f\"Based on your question: '{user_query}', and the following context: [{'; '.join(retrieved_context)}], the AI assistant says: This is a simulated response based on the relevant information found.\"\n",
        "\n",
        "    # Add escalation message if required by keywords\n",
        "    if escalation_required and not (not retrieved_context):\n",
        "        generated_answer = \"Escalation is required. \" + generated_answer\n",
        "    elif escalation_required and (not retrieved_context):\n",
        "        pass # Message already includes escalation due to no context\n",
        "\n",
        "    return generated_answer, source_ids, escalation_required\n",
        "\n",
        "print(\"The 'simulate_ask' function has been updated with escalation logic.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "036470a1"
      },
      "source": [
        "print(\"\\n--- Testing Escalation Logic ---\")\n",
        "\n",
        "# Test Case 1: Query with escalation keyword\n",
        "user_query_escalate_keyword = \"I have an urgent complaint about my service.\"\n",
        "answer_ek, sources_ek, escalate_ek = simulate_ask(user_query_escalate_keyword)\n",
        "print(f\"\\nUser Query (Keyword): {user_query_escalate_keyword}\")\n",
        "print(f\"Generated Answer: {answer_ek}\")\n",
        "print(f\"Source IDs: {sources_ek}\")\n",
        "print(f\"Escalation Required: {escalate_ek}\")\n",
        "\n",
        "# Test Case 2: Query with no relevant context (likely to trigger escalation if no context is found)\n",
        "user_query_no_context = \"Tell me about quantum physics and black holes.\"\n",
        "answer_nc, sources_nc, escalate_nc = simulate_ask(user_query_no_context)\n",
        "print(f\"\\nUser Query (No Context): {user_query_no_context}\")\n",
        "print(f\"Generated Answer: {answer_nc}\")\n",
        "print(f\"Source IDs: {sources_nc}\")\n",
        "print(f\"Escalation Required: {escalate_nc}\")\n",
        "\n",
        "# Test Case 3: Standard query without escalation\n",
        "user_query_normal = \"What is the customer's issue regarding porting out?\"\n",
        "answer_n, sources_n, escalate_n = simulate_ask(user_query_normal)\n",
        "print(f\"\\nUser Query (Normal): {user_query_normal}\")\n",
        "print(f\"Generated Answer: {answer_n}\")\n",
        "print(f\"Source IDs: {sources_n}\")\n",
        "print(f\"Escalation Required: {escalate_n}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1792d7dd"
      },
      "source": [
        "escalation_keywords = ['escalate', 'urgent', 'manager', 'complaint']\n",
        "\n",
        "def simulate_ask(user_query, top_k=3, relevance_threshold=0.7): # Added relevance_threshold parameter\n",
        "    escalation_required_by_keyword = False\n",
        "\n",
        "    # Check for escalation keywords in the user query\n",
        "    for keyword in escalation_keywords:\n",
        "        if keyword in user_query.lower():\n",
        "            escalation_required_by_keyword = True\n",
        "            break\n",
        "\n",
        "    # 1. Embed the user query\n",
        "    query_embedding = model.encode([user_query])\n",
        "    query_embedding = np.array(query_embedding).astype('float32')\n",
        "\n",
        "    # 2. Perform similarity search\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    # 3. Retrieve relevant text chunks (source IDs), filtering by relevance_threshold\n",
        "    retrieved_context = []\n",
        "    source_ids = []\n",
        "\n",
        "    for i, dist in zip(indices[0], distances[0]):\n",
        "        if dist < relevance_threshold and i < len(all_text_chunks): # Only add if distance is below threshold and index is valid\n",
        "            retrieved_context.append(all_text_chunks[i])\n",
        "            source_ids.append(i)\n",
        "\n",
        "    escalation_required_by_no_context = False\n",
        "    if not retrieved_context: # If no context found after filtering by relevance\n",
        "        escalation_required_by_no_context = True\n",
        "\n",
        "    final_escalation_status = escalation_required_by_keyword or escalation_required_by_no_context\n",
        "\n",
        "    # 4. Simulate LLM response based on context and escalation status\n",
        "    if escalation_required_by_no_context:\n",
        "        generated_answer = f\"Based on your question: '{user_query}', the AI assistant says: I couldn't find sufficiently relevant information in my knowledge base. This query requires escalation due to lack of relevant information.\"\n",
        "    elif retrieved_context:\n",
        "        generated_answer = f\"Based on your question: '{user_query}', and the following context: [{'; '.join(retrieved_context)}], the AI assistant says: This is a simulated response based on the relevant information found.\"\n",
        "    else: # Fallback, though ideally should be caught by `escalation_required_by_no_context`\n",
        "        generated_answer = f\"Based on your question: '{user_query}', the AI assistant says: I couldn't process your request.\"\n",
        "\n",
        "    # Prepend escalation message if keywords triggered it and not already handled by no-context message\n",
        "    if escalation_required_by_keyword and not escalation_required_by_no_context:\n",
        "        generated_answer = \"Escalation is required (keywords). \" + generated_answer\n",
        "    elif escalation_required_by_keyword and escalation_required_by_no_context:\n",
        "        # If both trigger, combine the message\n",
        "        generated_answer = generated_answer.replace(\n",
        "            \"This query requires escalation due to lack of relevant information.\",\n",
        "            \"This query requires escalation due to keywords and lack of relevant information.\"\n",
        "        )\n",
        "        generated_answer = \"Escalation is required (keywords and no relevant context). \" + generated_answer\n",
        "\n",
        "\n",
        "    return generated_answer, source_ids, final_escalation_status\n",
        "\n",
        "print(\"The 'simulate_ask' function has been updated with refined escalation logic and relevance threshold.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63422730"
      },
      "source": [
        "print(\"\\n--- Retesting Refined Escalation Logic ---\")\n",
        "\n",
        "# Test Case 1: Query with escalation keyword (should still escalate)\n",
        "user_query_escalate_keyword = \"I have an urgent complaint about my service.\"\n",
        "answer_ek, sources_ek, escalate_ek = simulate_ask(user_query_escalate_keyword)\n",
        "print(f\"\\nUser Query (Keyword): {user_query_escalate_keyword}\")\n",
        "print(f\"Generated Answer: {answer_ek}\")\n",
        "print(f\"Source IDs: {sources_ek}\")\n",
        "print(f\"Escalation Required: {escalate_ek}\")\n",
        "\n",
        "# Test Case 2: Query with no relevant context (should now trigger escalation with relevance_threshold)\n",
        "# Adjust relevance_threshold to be more stringent if previous attempts didn't trigger 'no context'\n",
        "# For example, if a distance of 0.7 was too high, try 0.5 or 0.3. Let's start with 0.5 for demonstration.\n",
        "user_query_no_context = \"Tell me about quantum physics and black holes. This is urgent.\"\n",
        "answer_nc, sources_nc, escalate_nc = simulate_ask(user_query_no_context, relevance_threshold=0.5)\n",
        "print(f\"\\nUser Query (No Context, stricter threshold): {user_query_no_context}\")\n",
        "print(f\"Generated Answer: {answer_nc}\")\n",
        "print(f\"Source IDs: {sources_nc}\")\n",
        "print(f\"Escalation Required: {escalate_nc}\")\n",
        "\n",
        "# Test Case 3: Standard query without escalation (should still not escalate)\n",
        "user_query_normal = \"What is the customer's issue regarding porting out?\"\n",
        "answer_n, sources_n, escalate_n = simulate_ask(user_query_normal)\n",
        "print(f\"\\nUser Query (Normal): {user_query_normal}\")\n",
        "print(f\"Generated Answer: {answer_n}\")\n",
        "print(f\"Source IDs: {sources_n}\")\n",
        "print(f\"Escalation Required: {escalate_n}\")\n",
        "\n",
        "# Test Case 4: Combined escalation - keyword AND no relevant context (using a very out-of-domain query)\n",
        "user_query_combined_escalation = \"I need to escalate this matter about alien abductions, it's urgent!\"\n",
        "answer_ce, sources_ce, escalate_ce = simulate_ask(user_query_combined_escalation, relevance_threshold=0.5)\n",
        "print(f\"\\nUser Query (Combined Escalation): {user_query_combined_escalation}\")\n",
        "print(f\"Generated Answer: {answer_ce}\")\n",
        "print(f\"Source IDs: {sources_ce}\")\n",
        "print(f\"Escalation Required: {escalate_ce}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7435b5cc"
      },
      "source": [
        "print(\"\\n--- Demonstrating AI Agent Functionality ---\")\n",
        "\n",
        "# Test Case 1: Query that should result in a direct answer with relevant sources\n",
        "user_query_direct_answer = \"What is the customer's issue regarding porting out?\"\n",
        "answer_da, sources_da, escalate_da = simulate_ask(user_query_direct_answer)\n",
        "print(f\"\\nScenario: Direct Answer with Relevant Sources\")\n",
        "print(f\"User Query: {user_query_direct_answer}\")\n",
        "print(f\"Generated Answer: {answer_da}\")\n",
        "print(f\"Source IDs: {sources_da}\")\n",
        "print(f\"Escalation Required: {escalate_da}\")\n",
        "\n",
        "# Test Case 2: Query containing an escalation keyword\n",
        "user_query_keyword_escalation = \"I need to speak to a manager about my bill.\"\n",
        "answer_ke, sources_ke, escalate_ke = simulate_ask(user_query_keyword_escalation)\n",
        "print(f\"\\nScenario: Query with Escalation Keyword\")\n",
        "print(f\"User Query: {user_query_keyword_escalation}\")\n",
        "print(f\"Generated Answer: {answer_ke}\")\n",
        "print(f\"Source IDs: {sources_ke}\")\n",
        "print(f\"Escalation Required: {escalate_ke}\")\n",
        "\n",
        "# Test Case 3: Query for which no relevant information exists (should trigger no-context escalation)\n",
        "user_query_no_info = \"Explain the theory of relativity to me.\"\n",
        "answer_ni, sources_ni, escalate_ni = simulate_ask(user_query_no_info, relevance_threshold=0.5) # Using a stricter threshold to ensure no context\n",
        "print(f\"\\nScenario: Query with No Relevant Information\")\n",
        "print(f\"User Query: {user_query_no_info}\")\n",
        "print(f\"Generated Answer: {answer_ni}\")\n",
        "print(f\"Source IDs: {sources_ni}\")\n",
        "print(f\"Escalation Required: {escalate_ni}\")\n",
        "\n",
        "# Test Case 4: Query with both an escalation keyword and no relevant information\n",
        "user_query_combined_escalation = \"This is urgent, I need a manager to resolve my issue with intergalactic travel services!\"\n",
        "answer_ce, sources_ce, escalate_ce = simulate_ask(user_query_combined_escalation, relevance_threshold=0.5) # Using a stricter threshold\n",
        "print(f\"\\nScenario: Combined Escalation (Keyword + No Relevant Info)\")\n",
        "print(f\"User Query: {user_query_combined_escalation}\")\n",
        "print(f\"Generated Answer: {answer_ce}\")\n",
        "print(f\"Source IDs: {sources_ce}\")\n",
        "print(f\"Escalation Required: {escalate_ce}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}